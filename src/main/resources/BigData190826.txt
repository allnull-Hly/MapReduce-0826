魏 运慧   

课程安排:   JavaWeb(4天) +  Maven(1) + SSM+SpringBoot(2天) +  Linux(2天) + Redis (2天) +  
	     MySql高级(2天)+ JVM(1) + JUC(1) + Git(1) + Shell(1) + Hadoop(9) + Zookeeper(1) +HA(1)

统一环境:   JDK8 + MySQL5.5以上 + Eclipse(STS) 


Redis:

Java中的 i++ i-- 操作是否为原子操作?   不是. 

面试题:  
int i = 0  , 有两个线程同时给i进行100次++操作，问 最后i = ?  2 ~ 200


Node: 
    private Node prev ; 
    private Object value ;
    private Node next ; 


Hash :
一个java对象如何在Redis中存储：

User ={id=1001 , name=张三, email=zs@sina.com , age =22 ,..... }

第一种方案: key(id) - value(user的json格式)

第二种方案: key(id+属性) - value(属性值)
	    1001:id  - 1001 
	    1001:name - 张三
	    1001:email - zs@sina.com
	    1001:age - 22 

第三种方案: key(id) - value(hash) 
	    1001  -  {id-1001 
		      name-张三
	              email-zs@sina.com
		      age-22
	             }	

PidFile:

执行一个脚本，将Redis的服务关闭 
pid = cat redis_6379.pid
kill -9  $pid 



ab:
ab –n 请求数  -c 并发数  -p  指定请求数据文件  -T  “application/x-www-form-urlencoded”  测试的请求

ab -n 400 -c 300 -p ~/postfile -T "application/x-www-form-urlencoded"  http://192.168.19.40:8888/seckill/doseckill 
 


AOF重写:

set balance 10000
set k1 v1 
decrby balance  1000
set k2 v2 
incrby balance 2000

set balance 11000
set k1 v1 
set k2 v2 



Redis集群:
./redis-trib.rb create --replicas 1 192.168.202.100:6379 192.168.202.100:6380  192.168.202.100:6381 192.168.202.100:6389 192.168.202.100:6390  192.168.202.100:6391



Mysql主从

GRANT REPLICATION SLAVE ON *.* TO 'slave101'@'192.168.202.101' IDENTIFIED BY '123456';

CHANGE MASTER TO MASTER_HOST='192.168.202.100',MASTER_USER='slave101',MASTER_PASSWORD='123456',MASTER_LOG_FILE='mysql-bin.000009',MASTER_LOG_POS=259;
	    


克隆虚拟机:
1. vim /etc/udev/rules.d/70-persistent-net.rules   修改网卡信息，拷贝mac地址
2. vim /etc/sysconfig/network-scripts/ifcfg-eth0   修改ip地址和mac地址
3. vim /etc/sysconfig/network  修改主机名
4. vim /etc/hosts  查看ip和主机的映射
5. 添加用户设置root权限:
   useradd  atguigu 
   passwd atguigu 
   vim /etc/sudoers  91行下面添加  atguigu ALL=(ALL)       NOPASSWD:ALL





Job提交流程(Job真正执行之前): 
1. if (state == JobState.DEFINE)   ==> submit();
   判断当前Job的状态是否为 DEFINE

2. submit():
   1). ensureState(JobState.DEFINE); 再次确认Job的状态
   2). setUseNewAPI(); 设置使用新的API
   3). connect(): 创建Cluster对象.
     ① return new Cluster(getConfiguration());
     ② initialize(jobTrackAddr, conf);	
        clientProtocol = provider.create(conf);  获取到当前运行环境 LocalJobRunner
  
	ClientProtocol:  LocalJobRunner | YarnRunner
  
   4). return submitter.submitJobInternal(Job.this, cluster);
       通过JobSumitter提交Job
     
      ① checkSpecs(job); 校验输出路径是否存在
      ② Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf); Job工作目录
      ③ JobID jobId = submitClient.getNewJobID(); 生成Jobid
      ④ Path submitJobDir = new Path(jobStagingArea, jobId.toString()); 
         file:/tmp/hadoop-Administrator/mapred/staging/Administrator148400293/.staging/job_local148400293_0001
      ⑤ copyAndConfigureFiles(job, submitJobDir); 生成job的工作目录
      ⑥ int maps = writeSplits(job, submitJobDir); 生成切片信息，返回切片的数量
	 [1]. maps = writeNewSplits(job, jobSubmitDir);
	 [2].InputFormat<?, ?> input =
		 ReflectionUtils.newInstance(job.getInputFormatClass(), conf);
             获取InputFormat对象，默认的是 TextInputFormat
	 [3].List<InputSplit> splits = input.getSplits(job); 
	     调用InputFormat的getSplits方法获取切面信息
	     A. long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));  ==>1
		long maxSize = getMaxSplitSize(job); ==>Long.MAX_VALUE
             B. long blockSize = file.getBlockSize(); 获取块大小，本地块大小为32M.
		long splitSize = computeSplitSize(blockSize, minSize, maxSize); 计算切片大小
		Math.max(minSize, Math.min(maxSize, blockSize));
	     C. (double) bytesRemaining)/splitSize > SPLIT_SLOP(1.1) 计算是否需要继续切         
	
      ⑦  conf.setInt(MRJobConfig.NUM_MAPS, maps); 根据切片个数设置MapTask个数     
	  
      ⑧  writeConf(conf, submitJobFile);  将job执行的配置信息写入到工作目录中
         
      ⑨  status = submitClient.submitJob(
          jobId, submitJobDir.toString(), job.getCredentials());
	  通过LocalJobRunnber 真正的提交Job 

      ⑩  jtFs.delete(submitJobDir, true); Job执行结束后删除工作目录


MapTask:
1.  status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());
    通过LocalJobRunnber 真正的提交Job 
    1). Job job = new Job(JobID.downgrade(jobid), jobSubmitDir);
        构造一个真正执行的Job对象  LocalJobRunner.Job
	[1]. this.start(); 启动Job线程
	[2]. 执行LocalJobRunner.Job的run方法.
	     ①  List<RunnableWithThrowable> mapRunnables = getMapTaskRunnables(
                 taskSplitMetaInfos, jobId, mapOutputFiles);
		 获取当前的Job需要启动几个任务  LocalJobRunner$Job$MapTaskRunnable
	     ②  runTasks(mapRunnables, mapService, "map");
	         把MapTaskRunnable 交给线程池执行
		 A.  for (Runnable r : runnables) {
			service.submit(r);
		     } 
		     迭代所有的MapTaskRunnable ，提交到线程池执行.

		 B. 执行MapTaskRunnable中的run方法. 
		    
		    I: MapTask map = new MapTask(systemJobFile.toString(), mapId, taskId,
		      info.getSplitIndex(), 1);
		      创建MapTask对象
		    II: map.run(localConf, Job.this);
		       调用MapTask的run方法. 
		       [1]. runNewMapper(job, splitMetaInfo, umbilical, reporter);
		            执行MapTask的runNewMapper方法
		            ①  mapper.run(mapperContext);
			        执行Mapper中的run方法. 
		            ②  执行到xxxMapper中的map方法。		   

Shuffle:
一、 shuffle准备工作
1.  MapTask中的run方法中的runNewMapper()
    1).org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE> mapper =
      (org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>)
        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);
       获取Mapper对象
    2).org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE> inputFormat =
      (org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE>)
        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);
       获取到InputFormat对象
    3). org.apache.hadoop.mapreduce.RecordReader<INKEY,INVALUE> input =
      new NewTrackingRecordReader<INKEY,INVALUE>
        (split, inputFormat, reporter, taskContext);
       获取到RecordReader对象
    4).// get an output object
    if (job.getNumReduceTasks() == 0) {
      output = 
        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);
    } else {
      output = new NewOutputCollector(taskContext, job, umbilical, reporter);
    }
      根据Reduce的个数，获取收集器对象.用于收集Map写出的kv
       
      ① collector = createSortingCollector(job, reporter);
         创建收集器对象MapOutputCollector

	 [1]. Class<?>[] collectorClasses = job.getClasses(
		JobContext.MAP_OUTPUT_COLLECTOR_CLASS_ATTR, MapOutputBuffer.class);
	      获取到具体的收集器对象的类型 MapOutputBuffer
	 [2]. MapOutputCollector<KEY, VALUE> collector =
                  ReflectionUtils.newInstance(subclazz, job);
	      创建收集器对象
	 [3]. collector.init(context);  初始化收集器对象
	      A. final float spillper =
		 job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);
		 获取到溢写百分比
	      B. final int sortmb = job.getInt(JobContext.IO_SORT_MB, 100);
	         获取到缓冲区的大小
	      C. sorter = ReflectionUtils.newInstance(job.getClass("map.sort.class",
			  QuickSort.class, IndexedSorter.class), job);
	         获取到排序对象
              D. comparator = job.getOutputKeyComparator(); 获取分组比较器
	         1>.WritableComparator.get(getMapOutputKeyClass().asSubclass(WritableComparable.class), this);

	      E. compression  压缩相关的操作

	      F. combiner  合并

	      G. spillThread.start();  
	         启动溢写线程


二、 收集  排序   溢写  合并 
1. Mapper类中的  context.write(k, v);
2. mapContext.write(key, value);
3. output.write(key, value);
4. collector.collect(key, value,
                        partitioner.getPartition(key, value, partitions)); 
   通过MapOutputBuffer收集map写出的kv
   1).startSpill(); 满足溢写条件开始溢写
      [1]. spillReady.signal(); 通知开始进行溢写
   2).sortAndSpill(): 排序溢写
      [1].final Path filename =
            mapOutputFile.getSpillFileForWrite(numSpills, size); 获取溢写文件名
           out = rfs.create(filename);  创建溢写文件
      [2].sorter.sort(MapOutputBuffer.this, mstart, mend, reporter); 排序

      [3]. if (totalIndexCacheMemory >= indexCacheMemoryLimit) {
          // create spill index file
          Path indexFilename =
              mapOutputFile.getSpillIndexFileForWrite(numSpills, partitions
                  * MAP_OUTPUT_INDEX_RECORD_LENGTH);
          spillRec.writeToFile(indexFilename, job);

	  判断存放排好序的索引占用的内存是否大于等于阈值，如果满足，也需要在溢写到磁盘.

5. 在Mapper中所有的kv全部写出去以后，会执行 runNewMapper中的 output.close(mapperContext);
   1).  collector.flush();
   2).  sortAndSpill();
   3).  mergeParts();
	[1].Path finalOutputFile =
             mapOutputFile.getOutputFileForWrite(finalOutFileSize);
	     获取最终合并完的溢写文件名  file.out
	   Path finalIndexFile =
           mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);
	     获取最终溢写到磁盘的索引文件  file.out.index
	[2]. Merger.writeFile(kvIter, writer, reporter, job);
	     将合并好的数据写到最终的文件中
	[3]. spillRec.writeToFile(finalIndexFile, job);
	     将索引文件也写到文件中
	[4].  rfs.delete(filename[i],true);
	     将溢写文件删除

ReduceTask:
1. List<RunnableWithThrowable> reduceRunnables = getReduceTaskRunnables(
                jobId, mapOutputFiles);
   根据Job信息获取到 LocalJobRunner$Job$ReduceTaskRunnable
2. runTasks(reduceRunnables, reduceService, "reduce");
   把ReduceTaskRunnable 交给线程池去执行
   1). for (Runnable r : runnables) {
        service.submit(r);
      }
      迭代出每一个ReduceTaskRunnable 提交到线程池执行
      [1] . 执行ReduceTaskRunnable 的run方法. 
          ① ReduceTask reduce = new ReduceTask(systemJobFile.toString(),
              reduceId, taskId, mapIds.size(), 1);
	     创建ReduceTask对象
          ②  reduce.run(localConf, Job.this);
	      执行ReduceTask中的run方法
	  ③ runNewReducer(job, umbilical, reporter, rIter, comparator, 
                    keyClass, valueClass);
             A. reducer.run(reducerContext); 执行Reducer中的run方法
	     B. 执行xxxReducer中的reduce方法。



InputFormat:
     重要方法:
	getSplits(): 生成切片信息
	createRecordReader(): 获取RecordReader来负责数据的读取
     子抽象类: 
	FileInputFormat
            重要方法: 
		isSplitable():是否可切分
     具体实现类: 
	TextInputFormat 默认的
	CombineTextInputFormat
	KeyValueTextInputFormat
	NLineInputFormat

OutputFormat
     重要方法: 
	getRecordWriter(): 获取RecordWriter来负责数据的写出
     子抽象类:
	FileOutputFormat
     具体实现类:
	SequenceFileOutputFormat
	TextOutputFormat




Partitioner 分区器
    默认的分区器  HashPartitioner
    public int getPartition(K key, V value,
                          int numReduceTasks) {
       return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;
    }

    如何确定每个Key的分区:
     partitions = jobContext.getNumReduceTasks();
      if (partitions > 1) {
        partitioner = (org.apache.hadoop.mapreduce.Partitioner<K,V>)
          ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);
      } else {
        partitioner = new org.apache.hadoop.mapreduce.Partitioner<K,V>() {
          @Override
          public int getPartition(K key, V value, int numPartitions) {
            return partitions - 1;
          }
        };
      }